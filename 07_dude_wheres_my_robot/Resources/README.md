# 07_dude_wheres_my_robot
  The robot.txt contains a list of folders and files web crawlers can/can't access from your site

## Method
Go to http://192.168.1.75/robots.txt  

## How to exploit
  run the script.sh file and pass in the ip address of the site
  The script will use wget a webcrawler download all files and directories
  and find the flag using the find command

## Solution
- Use a router on the frontend so users have no access to file system
